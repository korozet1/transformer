# âš¡ Transformer-PyTorch-Deep-Dive (æºç æ·±åº¦è§£æ)

![Python](https://img.shields.io/badge/Python-3.8%2B-blue)
![PyTorch](https://img.shields.io/badge/PyTorch-1.10%2B-ee4c2c)
![Architecture](https://img.shields.io/badge/Architecture-Encoder--Decoder-success)
![Status](https://img.shields.io/badge/Status-Research_&_Education-purple)

> **"What I cannot create, I do not understand."** â€” Richard Feynman

## ğŸ“– é¡¹ç›®èƒŒæ™¯ (Introduction)

æœ¬é¡¹ç›®æ˜¯ Google ç»å…¸è®ºæ–‡ **[Attention Is All You Need](https://arxiv.org/abs/1706.03762)** çš„ **PyTorch åŸç”Ÿå¤ç°ä¸æºç çº§æ·±åº¦è§£æ**ã€‚

ä¸åŒäºç›´æ¥è°ƒç”¨ `torch.nn.Transformer` APIï¼Œæœ¬é¡¹ç›®**æ‰‹åŠ¨æ„å»ºäº† Transformer çš„æ¯ä¸€ä¸ªç»„ä»¶**ï¼ˆEmbedding, Positional Encoding, Multi-Head Attention, LayerNorm, FeedForwardï¼‰ï¼Œæ—¨åœ¨ä»åº•å±‚ Tensor å˜æ¢çš„è§’åº¦ï¼Œå½»åº•è§£æ„å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„åŸºçŸ³ã€‚

ä»£ç æ ¸å¿ƒé€»è¾‘åŸºäº **é»‘é©¬ç¨‹åºå‘˜** è¯¾ç¨‹å®ç°ï¼Œåœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘è¿›è¡Œäº†**æ¶æ„é‡æ„ã€è¯¦ç»†æ³¨é‡Šæ³¨å…¥ä»¥åŠæ•°å­¦åŸç†å¯¹åº”**ï¼Œä½¿å…¶æ›´é€‚åˆä½œä¸ºæ·±åº¦å­¦ä¹ è¿›é˜¶çš„å­¦ä¹ èµ„æ–™ã€‚

---

## ğŸ—ï¸ é¡¹ç›®ç»“æ„ (Directory Structure)

```text
Transformer-Deep-Dive/
â”œâ”€â”€ dm01_input.py           # [Input] è¯åµŒå…¥(Embedding) + ä½ç½®ç¼–ç (PE)
â”œâ”€â”€ dm02_encoder_element.py # [Core] å¤šå¤´æ³¨æ„åŠ›(MHA) + æ©ç ç”Ÿæˆ(Mask) + ç¼©æ”¾ç‚¹ç§¯
â”œâ”€â”€ dm03_encoder_sublayer.py# [Block] æ®‹å·®è¿æ¥(Residual) + å±‚å½’ä¸€åŒ–(LayerNorm)
â”œâ”€â”€ dm04_encoder_layer.py   # [Layer] å•å±‚ Encoder ç»„è£…
â”œâ”€â”€ dm05_encoder.py         # [Module] å®Œæ•´ Encoder å †å  (N=6)
â”œâ”€â”€ dm06_decoder_layer.py   # [Layer] å•å±‚ Decoder (å« Cross-Attention)
â”œâ”€â”€ dm07_decoder.py         # [Module] å®Œæ•´ Decoder å †å  (N=6)
â”œâ”€â”€ dm08_output.py          # [Output] çº¿æ€§å±‚ + Softmax ç”Ÿæˆæ¦‚ç‡
â””â”€â”€ dm09_transformer.py     # [Main]  Transformer æ•´ä½“æ¶æ„ç»„è£…ä¸æµ‹è¯•
```

---

## ğŸ§  æ ¸å¿ƒæŠ€æœ¯æ·±åº¦è§£æ (Technical Deep Dive)

### 1. ä½ç½®ç¼–ç  (Positional Encoding)
ç”±äº Transformer å®Œå…¨åŸºäºæ³¨æ„åŠ›æœºåˆ¶ï¼Œä¸å…·å¤‡ RNN çš„æ—¶åºå½’çº³åç½®ï¼Œå› æ­¤å¿…é¡»æ˜¾å¼æ³¨å…¥ä½ç½®ä¿¡æ¯ã€‚æœ¬é¡¹ç›®å®ç°äº†è®ºæ–‡ä¸­çš„**æ­£å¼¦/ä½™å¼¦é¢‘ç‡ç¼–ç **ï¼š

$$PE_{(pos, 2i)} = \sin(pos / 10000^{2i/d_{model}})$$
$$PE_{(pos, 2i+1)} = \cos(pos / 10000^{2i/d_{model}})$$

* **ä»£ç å¯¹åº”**: `dm01_input.py` -> `PositionEncoding` ç±»
* **å…³é”®å®ç°**: ä½¿ç”¨ `div_term` è®¡ç®—é¢‘ç‡è¡°å‡ï¼Œé€šè¿‡ `register_buffer` å°† PE æ³¨å†Œä¸ºéå‚æ•°å¸¸é‡ã€‚

### 2. ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ› (Scaled Dot-Product Attention)
è¿™æ˜¯ Transformer çš„æ ¸å¿ƒå¼•æ“ã€‚ä¸ºäº†é˜²æ­¢ $d_k$ (ç»´åº¦) è¿‡å¤§å¯¼è‡´ç‚¹ç§¯ç»“æœæ¨å‘ Softmax çš„é¥±å’ŒåŒºï¼ˆæ¢¯åº¦æ¶ˆå¤±ï¼‰ï¼Œå¼•å…¥äº†ç¼©æ”¾å› å­ $\sqrt{d_k}$ã€‚

$$Attention(Q, K, V) = \text{softmax}(\frac{QK^T}{\sqrt{d_k}})V$$

* **ä»£ç å¯¹åº”**: `dm02_encoder_element.py` -> `attention()` å‡½æ•°
* **Mask æœºåˆ¶**: åœ¨è®¡ç®— Softmax ä¹‹å‰ï¼Œä½¿ç”¨ `masked_fill(mask == 0, -1e9)` å°†éœ€è¦æ©ç›–çš„ä½ç½®ï¼ˆå¦‚ Padding æˆ– Decoder çš„æœªæ¥ä¿¡æ¯ï¼‰ç½®ä¸ºè´Ÿæ— ç©·ã€‚

### 3. å¤šå¤´æ³¨æ„åŠ› (Multi-Head Attention)
é€šè¿‡å°†æ¨¡å‹æŠ•å½±åˆ°ä¸åŒçš„å­ç©ºé—´ï¼ˆSubspacesï¼‰ï¼Œè®©æ¨¡å‹èƒ½å¤ŸåŒæ—¶å…³æ³¨ä¸åŒä½ç½®çš„ç‰¹å¾ä¿¡æ¯ã€‚

$$MultiHead(Q, K, V) = Concat(head_1, ..., head_h)W^O$$
$$\text{where } head_i = Attention(QW_i^Q, KW_i^K, VW_i^V)$$

* **ä»£ç å¯¹åº”**: `dm02_encoder_element.py` -> `MultiHeadAttention` ç±»
* **å®ç°ç»†èŠ‚**: é‡‡ç”¨äº† `view` å’Œ `transpose` æ“ä½œå®ç° Heads çš„å¹¶è¡Œè®¡ç®—ï¼Œè€Œéå¾ªç¯å®ç°ï¼Œæå¤§æå‡äº†è®­ç»ƒæ•ˆç‡ã€‚

### 4. æ©ç ç­–ç•¥ (Masking Strategy)
æœ¬é¡¹ç›®å®ç°äº†ä¸¤ç§å…³é”®æ©ç ï¼š
* **Padding Mask**: ç”¨äºå¤„ç†å˜é•¿åºåˆ—ï¼Œå¿½ç•¥ `<pad>` tokenã€‚
* **Subsequent Mask (Look-ahead Mask)**: ç”¨äº Decoder è®­ç»ƒé˜¶æ®µã€‚é€šè¿‡**ä¸Šä¸‰è§’çŸ©é˜µ**å¼ºåˆ¶æ¨¡å‹åªèƒ½åˆ©ç”¨å½“å‰åŠä¹‹å‰çš„ Token è¿›è¡Œé¢„æµ‹ï¼Œä¿æŒè‡ªå›å½’ï¼ˆAuto-regressiveï¼‰ç‰¹æ€§ã€‚

---

## ğŸ› ï¸ å¿«é€Ÿå¼€å§‹ (Quick Start)

### ç¯å¢ƒä¾èµ–
* Python 3.8+
* PyTorch 1.10+
* NumPy

### è¿è¡Œå®Œæ•´æ¨¡å‹æµ‹è¯•
æ‰§è¡Œä¸»ç¨‹åºï¼Œè§‚å¯Ÿ Tensor åœ¨ Encoder å’Œ Decoder ä¸­çš„æµåŠ¨ä¸å½¢çŠ¶å˜åŒ–ï¼š

```bash
python dm09_transformer.py
```

**é¢„æœŸè¾“å‡º**:
```text
my_transformer: EncoderDecoder(...)
resultæ˜¯Transformerçš„è¾“å‡ºç»“æœ: tensor(...)
resultçš„(å½¢çŠ¶): torch.Size([2, 4, 1000]) 
# [Batch_Size, Seq_Len, Vocab_Size]
```

---

## ğŸ“ ä¸ªäººç ”è¯»ç¬”è®° (Study Notes)

åœ¨å¤ç°è¿‡ç¨‹ä¸­ï¼Œæˆ‘å¯¹ä»¥ä¸‹å‡ ä¸ªæ¶æ„ç»†èŠ‚æœ‰äº†æ›´æ·±çš„ç†è§£ï¼š

1.  **Why LayerNorm?** Transformer é€‰æ‹©äº† LayerNorm è€Œé BatchNormï¼Œå› ä¸º NLP æ•°æ®çš„ Seq_Len å¾€å¾€ä¸ä¸€è‡´ï¼ŒBN åœ¨å˜é•¿åºåˆ—ä¸Šè¡¨ç°ä¸ä½³ã€‚ä»£ç åœ¨æ¯ä¸ª `SublayerConnection` ä¸­éƒ½ä½¿ç”¨äº† LN (`dm03_encoder_sublayer.py`)ã€‚

2.  **Decoder çš„ "Shifted Right"** è®­ç»ƒæ—¶ Decoder çš„è¾“å…¥éœ€è¦å‘å³ç§»åŠ¨ä¸€ä½ï¼ˆå³ `<Start>` ç¬¦å·èµ·å§‹ï¼‰ï¼Œè¿™æ˜¯ä¸ºäº†é…åˆ Mask æœºåˆ¶ï¼Œç¡®ä¿æ¨¡å‹æ˜¯åœ¨â€œé¢„æµ‹â€ä¸‹ä¸€ä¸ªè¯ï¼Œè€Œä¸æ˜¯â€œçœ‹åˆ°â€äº†ä¸‹ä¸€ä¸ªè¯ã€‚

3.  **Cross-Attention çš„äº¤äº’é€»è¾‘** åœ¨ `dm06_decoder_layer.py` ä¸­ï¼Œå¯ä»¥æ¸…æ™°çœ‹åˆ° Decoder çš„ `Q` æ¥è‡ªè‡ªèº«ï¼Œè€Œ `K, V` æ¥è‡ª Encoder çš„è¾“å‡ºã€‚è¿™æœ¬è´¨ä¸Šæ˜¯ä¸€ä¸ª**Query-Retrieval**ï¼ˆæŸ¥è¯¢-æ£€ç´¢ï¼‰è¿‡ç¨‹ã€‚

---

## ğŸ¤ è‡´è°¢ & å¼•ç”¨

* **Original Paper**: [Vaswani et al., 2017](https://arxiv.org/abs/1706.03762)
* **Code Reference**: æ„Ÿè°¢ **é»‘é©¬ç¨‹åºå‘˜** æä¾›çš„åŸºç¡€æ•™å­¦ä»£ç ï¼Œä¸ºæœ¬é¡¹ç›®çš„æ·±åº¦åˆ†ææä¾›äº†è‰¯å¥½çš„èµ·æ­¥æ¡†æ¶ã€‚

---

**Author**: [korozet1](https://github.com/korozet1)  
**Profile**: CS Graduate Student | CV & NLP Researcher
